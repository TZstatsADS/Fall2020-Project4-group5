---
title: "Testing Report"
author: "Xujie Ma, Yiqi Lei, Xinyi Zhang, Jiaqi Yuan"
date: "11/26/2020"
output:
  pdf_document: default
  word_document: default
  html_document: default
---

In this notebook, we are presenting 3 algorithms: 
1. 14 A3+P5 Doubly Robust Estimation + boosted stumps 
2. 21 A6+P5 Regression Adjustment + boosted stumps
3. 15 A4    Regression Estimate

```{r setup,warning=FALSE,message=FALSE}
library(gbm)
library(dplyr)
```

## Data Import
```{r import data}
high <- read.csv('../data/highDim_dataset.csv')
low <- read.csv('../data/lowDim_dataset.csv')
```

# Algo 1: 14 A3+P5 Doubly Robust Estimation + boosted stumps 
## highDim_dataset
```{r}
# train-test split
n <- nrow(high)
n_train <- round(n*(4/5),0)
train_idx <- sample(1:n,n_train)
train_high <- high[train_idx,]
test_high <- high[-train_idx,]
```

Split treatment and control group, and complete regression for each group.
```{r}
treatment.group.high<-high[high$A==1,-2]
control.group.high<-high[high$A==0,-2]

treatment.model.high<-lm(Y~.,data=treatment.group.high)
control.model.high<-lm(Y~.,data=control.group.high)
```

Estimate m1(X) and m0(X) for all entries.
```{r}
X.high<-high[-c(1,2)]

high$m1<-predict(treatment.model.high,X.high)
high$m0<-predict(control.model.high,X.high)
```

Get propensity score for all entries using boosted stumps (Gradient Boosting Machine).

Using grid search to get proper parameters for gbm.
```{r Grid Search for highdim,message=FALSE,eval=FALSE}
# grid search
hyper_grid_high1 <- expand.grid(
  n.trees = c(40,50,60),
  shrinkage = c(.01, .05, .1),
  n.minobsinnode = c(5, 10, 15),
  bag.fraction = c(.65, .8, 1), 
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)

# randomize data
random_index <- sample(1:nrow(train_high), nrow(train_high))
random_ames_train <- train_high[random_index, ]

# grid search 
for(i in 1:nrow(hyper_grid_high1)) {
  # reproducibility
  set.seed(2020)
  # train model
  gbm.tune <- gbm(
    formula = A~.,
    distribution = "bernoulli",
    data = train_high[-1],
    n.trees = hyper_grid_high1$n.trees[i],
    interaction.depth = 1,
    shrinkage = hyper_grid_high1$shrinkage[i],
    n.minobsinnode = hyper_grid_high1$n.minobsinnode[i],
    bag.fraction = hyper_grid_high1$bag.fraction[i],
    train.fraction = .75
  )
  
  # add min training error and trees to grid
  hyper_grid_high1$optimal_trees[i] <- which.min(gbm.tune$valid.error)
  hyper_grid_high1$min_RMSE[i] <- sqrt(min(gbm.tune$valid.error))
}

hyper_grid_high1 %>% 
  dplyr::arrange(min_RMSE) %>%
  head(10)
```

Apply the parameters with min_RMSE (n.trees=60, shrinkage=0.1, n.minobsinnode=10, bag.fraction=1).
```{r,message=F}
set.seed(2020) 

tm_highe1 <- system.time(
  boost.high<-gbm(A~., data = train_high[-1], 
                  distribution = "bernoulli",
                   n.trees = 60, # the number of trees
                   shrinkage = 0.1, # learning rate
                   interaction.depth = 1, # total split
                  n.minobsinnode = 10,
                  bag.fraction = 1
                   )
  )

```



Calculate propensity scores for all entries in high.csv
```{r}

tm_highe2 <- system.time(
  high$e <- predict(boost.high, X.high, n.trees = 60, type = 'response')
)

```

Calculate each part in doubly robust estimation and count out the final result.
```{r}

tm_highATE1 <- system.time(
  {high$p1<-ifelse(high$A==1,(high$Y-high$m1)/high$e,0);
  high$p2<-ifelse(high$A==0,(high$Y-high$m0)/(1-high$e),0);
  high$result<-high$m1-high$m0+high$p1-high$p2;
  ATE.high<-mean(high$result)}
  )

ATE.high

#alternative function, same result

#tm_highATE2 <- system.time(
#  ATE.high<-1/n*(sum((high$A*high$Y-(high$A-high$e)*high$m1)/high$e)
#            -sum(((1-high$A)*high$Y+(high$A-high$e)*high$m0)/(1-high$e))))
#ATE.high
```

```{r}
# True ATE:
true_ATE_high <- -3

# Comparison:
true_ATE_high - ATE.high
```

```{r}
time_high<-tm_highe1[1]+tm_highe2[1]+tm_highATE1[1]
cat("Time for training gbm=", tm_highe1[1], "s \n")
cat("Time for getting propensity score=", tm_highe2[1], "s \n")
cat("Time for calculating ATE=", tm_highATE1[1], "s \n")
```



## lowDim_dataset
```{r}
# train-test split
n <- nrow(low)
n_train <- round(n*(4/5),0)
train_idx <- sample(1:n,n_train)
train_low <- low[train_idx,]
test_low <- low[-train_idx,]
```

Split treatment and control group, and complete regression for each group.
```{r}
treatment.group.low<-low[low$A==1,-2]
control.group.low<-low[low$A==0,-2]

treatment.model.low<-lm(Y~.,data=treatment.group.low)
control.model.low<-lm(Y~.,data=control.group.low)
```

Estimate m1(X) and m0(X) for all entries.
```{r}
X.low<-low[-c(1,2)]

low$m1<-predict(treatment.model.low,X.low)
low$m0<-predict(control.model.low,X.low)
```

Get propensity score for all entries using boosted stumps (Gradient Boosting Machine).

Using grid search to get proper parameters for gbm
```{r Grid Search for lowdim,message=FALSE,eval=FALSE}
# grid search
hyper_grid_low1 <- expand.grid(
  n.trees = c(40,50,60),
  shrinkage = c(.01, .05, .1),
  n.minobsinnode = c(5, 10, 15),
  bag.fraction = c(.65, .8, 1), 
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)

# randomize data
random_index <- sample(1:nrow(train_low), nrow(train_low))
random_ames_train <- train_low[random_index, ]

# grid search 
for(i in 1:nrow(hyper_grid_low1)) {
  # reproducibility
  set.seed(2020)
  # train model
  gbm.tune <- gbm(
    formula = A~.,
    distribution = "bernoulli",
    data = train_low[-1],
    n.trees = hyper_grid_low1$n.trees[i],
    interaction.depth = 1,
    shrinkage = hyper_grid_low1$shrinkage[i],
    n.minobsinnode = hyper_grid_low1$n.minobsinnode[i],
    bag.fraction = hyper_grid_low1$bag.fraction[i],
    train.fraction = 0.75
  )
  
  # add min training error and trees to grid
  hyper_grid_low1$optimal_trees[i] <- which.min(gbm.tune$valid.error)
  hyper_grid_low1$min_RMSE[i] <- sqrt(min(gbm.tune$valid.error))
}

hyper_grid_low1 %>% 
  dplyr::arrange(min_RMSE) %>%
  head(10)
```


Apply the parameters with min_RMSE (n.trees=60, shrinkage=0.1, n.minobsinnode=15, bag.fraction=0.8).
```{r,message=F}
set.seed(2020)

tm_lowe1 <- system.time(
boost.low <- gbm(A~., data = train_low[-1], 
                 distribution = "bernoulli",
                 n.trees = 60, # the number of trees
                 shrinkage = 0.1, # learning rate
                 interaction.depth = 1, # total split
                 n.minobsinnode = 15,
                 bag.fraction = 0.8
            )
)

```


Calculate propensity scores for all entries in high.csv
```{r}
tm_lowe2 <- system.time(
low$e <- predict(boost.low, X.low, n.trees = 60, type = 'response')
)
```

Calculate each part in doubly robust estimation and count out the final result.
```{r}

tm_lowATE1 <- system.time(
{low$p1<-ifelse(low$A==1,(low$Y-low$m1)/low$e,0);
low$p2<-ifelse(low$A==0,(low$Y-low$m0)/(1-low$e),0);
low$result<-low$m1-low$m0+low$p1-low$p2;
ATE.low<-mean(low$result)}
)

ATE.low

#alternative function, same result

#tm_lowATE2 <- system.time(
#ATE.low <- 1/n*(sum((low$A*low$Y-(low$A-low$e)*low$m1)/low$e)
#          -sum(((1-low$A)*low$Y+(low$A-low$e)*low$m0)/(1-low$e))))
#ATE.low
```

```{r}
# True ATE:
true_ATE_low <- 2.5

# Comparison:
true_ATE_low - ATE.low
```

```{r}
time_low<-tm_lowe1[1]+tm_lowe2[1]+tm_lowATE1[1]
cat("Time for training gbm=", tm_lowe1[1], "s \n")
cat("Time for getting propensity score=", tm_lowe2[1], "s \n")
cat("Time for calculating ATE=", tm_lowATE1[1], "s \n")
```

## Conclusion for Doubly Robust Estimation
ATE Estimation precision for HighDim dataset and LowDim dataset is pretty similar. Running time for HighDim dataset is around 43 times that for LowDim dataset.
```{r}
table<-data.frame(ATE=c(round(ATE.high,3),round(ATE.low,3)), True.ATE=c(true_ATE_high,true_ATE_low), 
                  diff=c(round(true_ATE_high - ATE.high,3),round(true_ATE_low - ATE.low,3)),
                  time=c(time_high,time_low),row.names = c('HighDim', 'LowDim'))
table
```

# Algo 2: 21 A6+P5 Regression Adjustment + boosted stumps

**The ATEs we got from boosted stumps and regression adjustment are 2.5271 and -3.083. We conclude that it is a close estimate where the true ATEs that are 2.5 and -3.**

## Methodology and Implementation
1. Reference: D'Agostino RB Jr. Propensity score methods for bias reduction in the comparison of a treatment to a non-randomized control group. Stat Med. 1998 Oct 15;17(19):2265-81. doi: 10.1002/(sici)1097-0258(19981015)17:19<2265::aid-sim918>3.0.co;2-b. PMID: 9802183.
2. Boosted stumps: 
- What is it: an ensemble of weak learners with boosting algorithms. We combine decision tree stumps (decision tree with depth of 1) to predict the propensity score of each sample to simulate a random sample in an observational setting. 
- Implementation: boost_low = gbm(A~., data = train_low[-1], 
            n.trees = 500, # the number of trees
            shrinkage = 0.03, # learning rate
            interaction.depth = 1, # depth of each tree, stumps
            cv.folds=5
            )
3. Regression adjustment: 
- What is it: regress the outcome variable Y on treatment indicator variable A and the estimated propensity score(pred_high); We use the estimated coefficient on the A as indicator variable as an estimate of ATE
- Implementation: ATE_high = lm(Y~A+pred_high,data=high)


## Data Preparation
```{r}
setwd('/Users/xinyi0351/Desktop/Fall 2020/5243/project 4/')
library(gbm)
library(caret)
high <- read.csv('highDim_dataset.csv')
low <- read.csv('lowDim_dataset.csv')
#high['A'] <- apply(high['A'],1,as.factor)
#low['A'] <- apply(low['A'],1,as.factor)
```

## Low Dimension
```{r cache=TRUE}
# trani-test split
set.seed(2021)
n <- nrow(low)
n_train <- round(n*(4/5),0)
train_idx <- sample(1:n,n_train)
# test_idx <- setdiff(1:2000, train)
train_low <- low[train_idx,]
test_low <- low[-train_idx,]


## Propensity Score
start0 <- Sys.time()

boost0 = gbm(A~., data = train_low[-1], 
            n.trees = 500, # the number of trees, 100, 1000. 10000, no big diff
            shrinkage = 0.03, # learning rate, 0.01, 0.03, 0.05, 0.1
            interaction.depth = 1 # depth of each tree, set 1 as stumps
            ) # here, the parameters we get are from grid search results - see the bottom of the file for detail

# n.trees <- seq(from = 100, to = 10000, by = 100)
# n.trees set the number of trees to be built. Here I choose 1000 manually.
pred0 <- predict(boost0, test_low[-c(1,2)],n.trees = 1000, type = 'response')

# plot by A to see the distribution of the predicted value
g0_index <- test_low$A == 0
g1_index <- test_low$A == 1
plot(density(pred0[g0_index]),col = 'red')
lines(density(pred0[g1_index]),col = 'blue')
legend('topright',legend = c('group 0','group 1'),fill = c('red','blue'))

## ATE

# build a regression model based on the propensity score
# structure the data frame
ps0 <- predict(boost0, low[-c(1,2)],n.trees = 100, type = 'response')
df0<-data.frame(low$Y,low$A, ps0)
colnames(df0) <- c('Y','A','PS')
model0<-lm(df0$Y~df0$A+df0$PS)

end0 <- Sys.time()
```

```{r summary 1}
summary(boost0)
summary(model0)
cat('The total time using boosted stumps and regression adjustment with low dimension data is:', end0 - start0,'s')
```

## High Dimension
```{r split}
# train-test split
set.seed(2021)
n <- nrow(high)
n_train <- round(n*(4/5),0)
train_idx <- sample(1:n,n_train)
# test_idx <- setdiff(1:2000, train)
train_high <- high[train_idx,]
test_high <- high[-train_idx,]


## Propensity Score
start1 <- Sys.time()

boost1 = gbm(A~., data = train_high[-1], 
            n.trees = 100, # the number of trees
            shrinkage = 0.001, # learning rate
            interaction.depth = 1 # stumps
            ) # here, the parameters we get are from grid search results - see the bottom of the file for detail

#n.trees <- seq(from = 100, to = 10000, by = 100)
# n.trees set the number of trees to be built. Here I choose 1000 manually.
pred1 <- predict(boost1, test_high[-c(1,2)],n.trees = 1000, type = 'response')
length(pred1)

# plot by A to see the distribution of the predicted value
g0_index <- test_high$A == 0
g1_index <- test_high$A == 1
plot(density(pred1[g0_index]),col = 'red')
lines(density(pred1[g1_index]),col = 'blue')
legend('topright',legend = c('group 0','group 1'),fill = c('red','blue'))

## ATE

# build a regression model based on the propensity score
# structure the data frame
ps1 <- predict(boost1, high[-c(1,2)],n.trees = 100, type = 'response')
df1<-data.frame(high$Y,high$A, ps1)
colnames(df1) <- c('Y','A','PS')
model1<-lm(df1$Y~df1$A+df1$PS)

end1 <- Sys.time()
```

```{r summary2}
summary(boost1)
summary(model1)
cat('The total time using boosted stumps and regression adjustment with high dimension data is:', end1 - start1,'s')
```

The summary of the model gives a feature importance plot. Conduct prediction on the test set so we can have Test Error as an evaluation. The density plot shows the overlap of propensity score between the two groups.


## Additional Code: Grid Search
In this part, we included our code for conducting grid search for lowdim. The procedure is similar in highdim. For the readability of the file, we choose to comment out the code. 
```{r Grid Search for lowdim}
# grid search
# hyper_grid_low1 <- expand.grid(
#  shrinkage = c(.01, 0.03, 0.05),
#  interaction.depth = 1 - since it is boosted stumps
#  n.minobsinnode = c(5, 10, 15),
#  bag.fraction = c(.65, .8, 1), 
#  optimal_trees = 0,               # a place to dump results
#  min_RMSE = 0                     # a place to dump results
#)

# randomize data
# random_index <- sample(1:nrow(train_low), nrow(train_low))
# random_ames_train <- train_low[random_index, ]

# grid search 
# for(i in 1:nrow(hyper_grid_low1)) {
  # reproducibility
#  set.seed(2020)
  # train model
#  gbm.tune <- gbm(
#    formula = A~.,
#    data = train_low[-1],
#    n.trees = 500,
#    interaction.depth = hyper_grid_low1$interaction.depth[i],
#    shrinkage = hyper_grid_low1$shrinkage[i],
#    n.minobsinnode = hyper_grid_low1$n.minobsinnode[i],
#  bag.fraction = hyper_grid_low1$bag.fraction[i],
#   train.fraction = .75,
#    n.cores = NULL, # will use all cores by default
#    verbose = FALSE
#  )

  # add min training error and trees to grid
#  hyper_grid_low1$optimal_trees[i] <- which.min(gbm.tune$valid.error)
#  hyper_grid_low1$min_RMSE[i] <- sqrt(min(gbm.tune$valid.error))
# }

# hyper_grid_low1 %>% 
# dplyr::arrange(min_RMSE) %>%
#  head(10)
```

