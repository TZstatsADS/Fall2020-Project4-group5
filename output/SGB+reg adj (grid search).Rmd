---
title: "SGB with Grid Search"
author: "Jiaqi Yuan"
date: "11/27/2020"
output: pdf_document
---

```{r setup}
library(dplyr)
library(gbm)
```

```{r setup}
# Import dataset
high <- read.csv('/Users/vikkiyuan/Desktop/highDim_dataset.csv')
low <- read.csv('/Users/vikkiyuan/Desktop/lowDim_dataset.csv')
```

# For Low
```{r train-test split for lowdim}
# train-test split for lowdim
n <- nrow(low)
n_train <- round(n*(4/5),0)
train_idx <- sample(1:n,n_train)
train_low <- low[train_idx,]
test_low <- low[-train_idx,]
```

## Take a initial look on how gbm fits 
```{r Take a initial look on how gbm fits}
set.seed(2020)
# see initial parameters
boost_low = gbm(A~., data = train_low[-1], 
            n.trees = 10000, # the number of trees
            shrinkage = 0.001, # learning rate
            interaction.depth = 1, # total split,
            cv.folds=5
            )
summary(boost_low)

# get MSE and compute RMSE
sqrt(min(boost_low$cv.error))

# plot loss function as a result of n trees added to the ensemble
gbm.perf(boost_low, method = "cv")
```

## Grid Search for lowdim
```{r Grid Search for lowdim}
# grid search
hyper_grid_low1 <- expand.grid(
  shrinkage = c(.01, .1, .3),
  interaction.depth = c(1, 3, 5),
  n.minobsinnode = c(5, 10, 15),
  bag.fraction = c(.65, .8, 1), 
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)

# randomize data
random_index <- sample(1:nrow(train_low), nrow(train_low))
random_ames_train <- train_low[random_index, ]

# grid search 
for(i in 1:nrow(hyper_grid_low1)) {
  # reproducibility
  set.seed(2020)
  # train model
  gbm.tune <- gbm(
    formula = A~.,
    data = train_low[-1],
    n.trees = 5000,
    interaction.depth = hyper_grid_low1$interaction.depth[i],
    shrinkage = hyper_grid_low1$shrinkage[i],
    n.minobsinnode = hyper_grid_low1$n.minobsinnode[i],
    bag.fraction = hyper_grid_low1$bag.fraction[i],
    train.fraction = .75,
    n.cores = NULL, # will use all cores by default
    verbose = FALSE
  )
  
  # add min training error and trees to grid
  hyper_grid_low1$optimal_trees[i] <- which.min(gbm.tune$valid.error)
  hyper_grid_low1$min_RMSE[i] <- sqrt(min(gbm.tune$valid.error))
}

hyper_grid_low1 %>% 
  dplyr::arrange(min_RMSE) %>%
  head(10)
```
## Refine model based on the grid search result for lowdim
```{r model after grid search for lowdim}
set.seed(2020)

boost_low_gs = gbm(A~., data = train_low[-1], 
            n.trees = 5000, # the number of trees
            shrinkage = 0.3, # learning rate
            interaction.depth = 3, # total split,
            cv.folds=5,
            n.minobsinnode = 5,
            bag.fraction = .65
            )
summary(boost_low_gs)
```
# Compute ATE for lowdim
```{r ATE lowdim}
pred_low <- predict(boost_low_gs, low[-c(1,2)],n.trees = 1000, type = 'response')
summary(pred_low)
ATE_low = lm(Y~A+pred_low,data=low)
ATE_low$coefficients["A"]
print(paste0("the ATE for low dimension data is ",ATE_low$coefficients["A"]))
```


# for High
```{r train-test split for highdim}
# train-test split
n <- nrow(high)
n_train <- round(n*(4/5),0)
train_idx <- sample(1:n,n_train)
train_high <- high[train_idx,]
test_high <- high[-train_idx,]
```

## Take a initial look on how gbm fits on high dim
```{r Take a initial look on how gbm fits on high dim}
set.seed(2020)
# see initial parameters
boost_high = gbm(A~., data = train_high[-1], 
            n.trees = 10000, # the number of trees
            shrinkage = 0.001, # learning rate
            interaction.depth = 1, # total split,
            cv.folds=5
            )
summary(boost_high)

# get MSE and compute RMSE
sqrt(min(boost_high$cv.error))

# plot loss function as a result of n trees added to the ensemble
gbm.perf(boost_high, method = "cv")

# grid search
hyper_grid <- expand.grid(
  shrinkage = c(.01, .1, .3,.5),
  interaction.depth = c(1, 3, 5),
  n.minobsinnode = c(5, 10, 15),
  bag.fraction = c(.65, .8, 1), 
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)

```

## Grid Search for high dim - take a long time
```{r Grid Search for high dim}
# randomize data
random_index <- sample(1:nrow(train_high), nrow(train_high))
random_ames_train <- train_high[random_index, ]

# grid search 
for(i in 1:nrow(hyper_grid)) {
  
  # reproducibility
  set.seed(2020)
  
  # train model
  gbm.tune <- gbm(
    formula = A~.,
    data = train_high[-1],
    n.trees = 5000,
    interaction.depth = hyper_grid$interaction.depth[i],
    shrinkage = hyper_grid$shrinkage[i],
    n.minobsinnode = hyper_grid$n.minobsinnode[i],
    bag.fraction = hyper_grid$bag.fraction[i],
    train.fraction = .75,
    n.cores = NULL, # will use all cores by default
    verbose = FALSE
  )
  
  # add min training error and trees to grid
  hyper_grid$optimal_trees[i] <- which.min(gbm.tune$valid.error)
  hyper_grid$min_RMSE[i] <- sqrt(min(gbm.tune$valid.error))
}

hyper_grid %>% 
  dplyr::arrange(min_RMSE) %>%
  head(10)
```
## Refine model based on the grid search result for high dim
```{r Refine model based on the grid search result for high dim}
set.seed(2020)

boost_high_gs = gbm(A~., data = train_high[-1], 
            n.trees = 5000, # the number of trees
            shrinkage = 0.3, # learning rate
            interaction.depth = 3, # total split,
            cv.folds=5,
            n.minobsinnode = 15,
            bag.fraction = .65
            )
summary(boost_high_gs)
```

# Compute ATE for highdim
```{r ATE high}
pred_high <- predict(boost_high_gs, high[-c(1,2)],n.trees = 1000, type = 'response')
summary(pred_high)
ATE_high = lm(Y~A+pred_high,data=high)
ATE_high$coefficients["A"]
print(paste0("the ATE for high dimension data is ",ATE_high$coefficients["A"]))
```
